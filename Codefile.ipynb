{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Start"
      ],
      "metadata": {
        "id": "s_nFjis7HvZX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ox8VCnjb0F0T",
        "outputId": "857b9898-42b9-42df-ad94-0a326de85b54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/train/\n",
        "!rm -rf /content/test/\n"
      ],
      "metadata": {
        "id": "XTND3wVWLcm8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q \"/content/drive/MyDrive/train.zip\" -d /content/train\n",
        "!unzip -q \"/content/drive/MyDrive/test.zip\" -d /content/test"
      ],
      "metadata": {
        "id": "HkJBrOmqDwmN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import layers\n",
        "import os"
      ],
      "metadata": {
        "id": "M_0rvz7c3K89"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run a basic ResNet50 model with transfer learning, first locking theta1 parameters and training for 5 epochs and then releasing the lock on theta1 and training for 5 epochs"
      ],
      "metadata": {
        "id": "x3FHrW_UH4zU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# ✅ Create proper structure\n",
        "os.makedirs(\"/content/train/cats\", exist_ok=True)\n",
        "os.makedirs(\"/content/train/dogs\", exist_ok=True)\n",
        "\n",
        "# ✅ Move images from /content/train/train/* → /content/train/cats|dogs/\n",
        "for filename in os.listdir(\"/content/train/train\"):\n",
        "    src_path = os.path.join(\"/content/train/train\", filename)\n",
        "    if os.path.isfile(src_path):\n",
        "        if filename.startswith(\"cat\"):\n",
        "            shutil.move(src_path, f\"/content/train/cats/{filename}\")\n",
        "        elif filename.startswith(\"dog\"):\n",
        "            shutil.move(src_path, f\"/content/train/dogs/{filename}\")\n",
        "\n",
        "# ✅ (Optional) Delete the now-empty /content/train/train/\n",
        "shutil.rmtree(\"/content/train/train\", ignore_errors=True)\n"
      ],
      "metadata": {
        "id": "sMJ3PVNB5LpP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data augmentation and rescaling\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.2,  # 80% training, 20% validation\n",
        "    rotation_range=15,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "# Training generator\n",
        "train_gen = train_datagen.flow_from_directory(\n",
        "    '/content/train',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "# Validation generator\n",
        "val_gen = train_datagen.flow_from_directory(\n",
        "    '/content/train',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    subset='validation'\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0Wxr9nR4K3b",
        "outputId": "72d9d6af-a559-46c3-b358-70218abfdc87"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20000 images belonging to 2 classes.\n",
            "Found 5000 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pretrained ResNet50 base\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze all layers up to 'conv1' (theta1)\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "    if 'conv1' in layer.name:\n",
        "        break\n",
        "\n",
        "# Add custom head\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "predictions = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train with theta1 frozen\n",
        "print(\"Training with theta1 frozen...\")\n",
        "history_phase1 = model.fit(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    epochs=5\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85-HfMce6sbO",
        "outputId": "04991ac8-0f17-4189-b248-2326ffd8e5ea"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with theta1 frozen...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m330s\u001b[0m 438ms/step - accuracy: 0.9411 - loss: 0.1411 - val_accuracy: 0.5298 - val_loss: 0.9689\n",
            "Epoch 2/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 431ms/step - accuracy: 0.9827 - loss: 0.0473 - val_accuracy: 0.9752 - val_loss: 0.0635\n",
            "Epoch 3/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m271s\u001b[0m 434ms/step - accuracy: 0.9864 - loss: 0.0371 - val_accuracy: 0.9584 - val_loss: 0.1249\n",
            "Epoch 4/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 430ms/step - accuracy: 0.9862 - loss: 0.0367 - val_accuracy: 0.9802 - val_loss: 0.0594\n",
            "Epoch 5/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 430ms/step - accuracy: 0.9907 - loss: 0.0294 - val_accuracy: 0.9752 - val_loss: 0.0727\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unfreeze all layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Recompile with smaller learning rate\n",
        "model.compile(optimizer=Adam(learning_rate=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Fine-tune the entire model\n",
        "print(\"Fine-tuning full network...\")\n",
        "history_phase2 = model.fit(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    epochs=5\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cq0o87Sf7INn",
        "outputId": "638683c4-44c1-47ea-c204-ab306413b4e7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuning full network...\n",
            "Epoch 1/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m334s\u001b[0m 445ms/step - accuracy: 0.9922 - loss: 0.0206 - val_accuracy: 0.9860 - val_loss: 0.0418\n",
            "Epoch 2/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m271s\u001b[0m 433ms/step - accuracy: 0.9966 - loss: 0.0121 - val_accuracy: 0.9856 - val_loss: 0.0414\n",
            "Epoch 3/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m270s\u001b[0m 432ms/step - accuracy: 0.9974 - loss: 0.0074 - val_accuracy: 0.9882 - val_loss: 0.0393\n",
            "Epoch 4/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m271s\u001b[0m 433ms/step - accuracy: 0.9981 - loss: 0.0053 - val_accuracy: 0.9876 - val_loss: 0.0429\n",
            "Epoch 5/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m270s\u001b[0m 431ms/step - accuracy: 0.9986 - loss: 0.0042 - val_accuracy: 0.9856 - val_loss: 0.0457\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Combine losses from both phases\n",
        "loss = history_phase1.history['loss'] + history_phase2.history['loss']\n",
        "val_loss = history_phase1.history['val_loss'] + history_phase2.history['val_loss']\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.title('Training vs Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "DVg-ReWU72E8",
        "outputId": "8c77a854-10fc-4bd8-c8b3-cfe99a06b174"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Create a dummy subfolder inside /content/test\n",
        "!mkdir -p /content/test/dummy\n",
        "!mv /content/test/*.jpg /content/test/dummy/\n"
      ],
      "metadata": {
        "id": "8jGZkMC9LF7q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ddca9bf-c39f-428c-e9c5-b37c475eecc9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot stat '/content/test/*.jpg': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/test/dummy"
      ],
      "metadata": {
        "id": "-18k1UMQZDK8"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    '/content/test',  # points to folder containing 'test' subfolder\n",
        "    target_size=(224, 224),\n",
        "    batch_size=1,\n",
        "    class_mode=None,\n",
        "    shuffle=False\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvBW6xbFKrAn",
        "outputId": "01dee534-8633-42d9-fab9-bd7102193947"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 12500 images belonging to 1 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict probabilities with your trained model\n",
        "predictions = model.predict(test_generator, verbose=1)\n",
        "preds_flat = predictions.ravel()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2x2QwtWLT9Y",
        "outputId": "aa4b0755-3760-4965-e9b1-e9dd3dc86d97"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m12500/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 6ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Extract file IDs (like 1, 10, 100) from filenames\n",
        "image_ids = [int(os.path.splitext(os.path.basename(path))[0]) for path in test_generator.filenames]\n",
        "\n",
        "# Create submission DataFrame\n",
        "submission_df = pd.DataFrame({\n",
        "    'id': image_ids,\n",
        "    'label': preds_flat\n",
        "})\n",
        "\n",
        "# Sort by ID to match Kaggle requirement\n",
        "submission_df = submission_df.sort_values('id')\n",
        "\n",
        "# Save CSV\n",
        "submission_df.to_csv('submission.csv', index=False)\n",
        "print(\"csv is ready\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmxaOgFtLeiB",
        "outputId": "710ddb85-3a59-4602-b368-a40f73f976a8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "csv is ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### For the above submission, I got a log loss on Kaggle of around 0.14. Good start for a baseline model and needed to be explored further"
      ],
      "metadata": {
        "id": "hIF4Q-kNNiTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32ypaWmTMhJx",
        "outputId": "8f4ff057-297c-454e-a6b9-1884c2ff74bb"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 204K\n",
            "drwx------ 7 root root 4.0K Apr 22 05:51 drive\n",
            "drwxr-xr-x 1 root root 4.0K Apr 17 13:36 sample_data\n",
            "-rw-r--r-- 1 root root 187K Apr 22 07:32 submission.csv\n",
            "drwxr-xr-x 3 root root 4.0K Apr 22 07:26 test\n",
            "drwxr-xr-x 4 root root 4.0K Apr 22 06:35 train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Trying out another approach with an ensemble of ResNet50, EfficientNetB0 and a custom CNN, to see if performance improves"
      ],
      "metadata": {
        "id": "1JPyaVcAONR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "\n",
        "def build_resnet50():\n",
        "    base = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "    x = GlobalAveragePooling2D()(base.output)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    output = Dense(1, activation='sigmoid')(x)\n",
        "    return Model(base.input, output)\n"
      ],
      "metadata": {
        "id": "iEoXdy8DQIL9"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "\n",
        "def build_efficientnet():\n",
        "    base = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "    x = GlobalAveragePooling2D()(base.output)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    output = Dense(1, activation='sigmoid')(x)\n",
        "    return Model(base.input, output)\n"
      ],
      "metadata": {
        "id": "MZHH1QknQdpB"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten\n",
        "\n",
        "def build_custom_cnn():\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
        "        MaxPooling2D(2, 2),\n",
        "        Conv2D(64, (3, 3), activation='relu'),\n",
        "        MaxPooling2D(2, 2),\n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "5lRAHpllQh0t"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_transfer_model(model_fn, name=\"model\"):\n",
        "    from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "    # 1. Build model\n",
        "    model = model_fn()\n",
        "\n",
        "    # 2. Freeze base layers\n",
        "    for layer in model.layers:\n",
        "        if hasattr(layer, 'trainable'):\n",
        "            layer.trainable = False\n",
        "        if 'conv1' in layer.name:  # Optionally unfreeze up to conv1\n",
        "            break\n",
        "\n",
        "    model.compile(optimizer=Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    print(f\"Training {name} with base frozen...\")\n",
        "    model.fit(train_gen, validation_data=val_gen, epochs=5)\n",
        "\n",
        "    # 3. Unfreeze all\n",
        "    for layer in model.layers:\n",
        "        if hasattr(layer, 'trainable'):\n",
        "            layer.trainable = True\n",
        "\n",
        "    model.compile(optimizer=Adam(1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    print(f\"Fine-tuning full {name} model...\")\n",
        "    model.fit(train_gen, validation_data=val_gen, epochs=5)\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "G1MXaXNio29F"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_model = train_transfer_model(build_resnet50, name=\"ResNet50\")\n",
        "effnet_model = train_transfer_model(build_efficientnet, name=\"EfficientNetB0\")\n",
        "\n",
        "custom_model = build_custom_cnn()\n",
        "custom_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "custom_model.fit(train_gen, validation_data=val_gen, epochs=5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqLVee4IQmS_",
        "outputId": "f50df93e-902f-47ae-f1f6-dea0d26d3f18"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training ResNet50 with base frozen...\n",
            "Epoch 1/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 441ms/step - accuracy: 0.9439 - loss: 0.1354 - val_accuracy: 0.7682 - val_loss: 0.4837\n",
            "Epoch 2/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m270s\u001b[0m 432ms/step - accuracy: 0.9819 - loss: 0.0501 - val_accuracy: 0.9758 - val_loss: 0.0718\n",
            "Epoch 3/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m271s\u001b[0m 433ms/step - accuracy: 0.9866 - loss: 0.0374 - val_accuracy: 0.9694 - val_loss: 0.0860\n",
            "Epoch 4/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 431ms/step - accuracy: 0.9903 - loss: 0.0290 - val_accuracy: 0.9750 - val_loss: 0.0675\n",
            "Epoch 5/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m270s\u001b[0m 432ms/step - accuracy: 0.9889 - loss: 0.0293 - val_accuracy: 0.9722 - val_loss: 0.0863\n",
            "Fine-tuning full ResNet50 model...\n",
            "Epoch 1/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 444ms/step - accuracy: 0.9929 - loss: 0.0183 - val_accuracy: 0.9852 - val_loss: 0.0401\n",
            "Epoch 2/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m271s\u001b[0m 434ms/step - accuracy: 0.9969 - loss: 0.0092 - val_accuracy: 0.9874 - val_loss: 0.0417\n",
            "Epoch 3/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m270s\u001b[0m 432ms/step - accuracy: 0.9971 - loss: 0.0082 - val_accuracy: 0.9870 - val_loss: 0.0398\n",
            "Epoch 4/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m270s\u001b[0m 432ms/step - accuracy: 0.9980 - loss: 0.0058 - val_accuracy: 0.9886 - val_loss: 0.0462\n",
            "Epoch 5/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m272s\u001b[0m 435ms/step - accuracy: 0.9983 - loss: 0.0054 - val_accuracy: 0.9886 - val_loss: 0.0446\n",
            "Training EfficientNetB0 with base frozen...\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py:82: UserWarning: The model does not have any trainable weights.\n",
            "  warnings.warn(\"The model does not have any trainable weights.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m291s\u001b[0m 442ms/step - accuracy: 0.4934 - loss: 0.7160 - val_accuracy: 0.5000 - val_loss: 0.7029\n",
            "Epoch 2/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 428ms/step - accuracy: 0.4968 - loss: 0.7150 - val_accuracy: 0.5000 - val_loss: 0.7029\n",
            "Epoch 3/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 430ms/step - accuracy: 0.4984 - loss: 0.7140 - val_accuracy: 0.5000 - val_loss: 0.7030\n",
            "Epoch 4/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 426ms/step - accuracy: 0.5046 - loss: 0.7127 - val_accuracy: 0.5000 - val_loss: 0.7030\n",
            "Epoch 5/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 424ms/step - accuracy: 0.4989 - loss: 0.7140 - val_accuracy: 0.5000 - val_loss: 0.7029\n",
            "Fine-tuning full EfficientNetB0 model...\n",
            "Epoch 1/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m350s\u001b[0m 447ms/step - accuracy: 0.7241 - loss: 0.5321 - val_accuracy: 0.8374 - val_loss: 0.3386\n",
            "Epoch 2/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m272s\u001b[0m 434ms/step - accuracy: 0.9495 - loss: 0.1524 - val_accuracy: 0.9162 - val_loss: 0.2173\n",
            "Epoch 3/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 430ms/step - accuracy: 0.9609 - loss: 0.1050 - val_accuracy: 0.9058 - val_loss: 0.2202\n",
            "Epoch 4/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 430ms/step - accuracy: 0.9696 - loss: 0.0780 - val_accuracy: 0.9600 - val_loss: 0.1038\n",
            "Epoch 5/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 430ms/step - accuracy: 0.9741 - loss: 0.0675 - val_accuracy: 0.9784 - val_loss: 0.0634\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 427ms/step - accuracy: 0.5800 - loss: 0.9349 - val_accuracy: 0.6618 - val_loss: 0.6229\n",
            "Epoch 2/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 428ms/step - accuracy: 0.6663 - loss: 0.6185 - val_accuracy: 0.6666 - val_loss: 0.6076\n",
            "Epoch 3/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 423ms/step - accuracy: 0.7117 - loss: 0.5619 - val_accuracy: 0.7536 - val_loss: 0.5138\n",
            "Epoch 4/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 430ms/step - accuracy: 0.7513 - loss: 0.5148 - val_accuracy: 0.7772 - val_loss: 0.4754\n",
            "Epoch 5/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 426ms/step - accuracy: 0.7684 - loss: 0.4847 - val_accuracy: 0.7800 - val_loss: 0.4627\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7eddd4987050>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset the test generator before each prediction\n",
        "test_generator.reset()\n",
        "resnet_preds = resnet_model.predict(test_generator, verbose=1)\n",
        "\n",
        "test_generator.reset()\n",
        "effnet_preds = effnet_model.predict(test_generator, verbose=1)\n",
        "\n",
        "test_generator.reset()\n",
        "custom_preds = custom_model.predict(test_generator, verbose=1)\n",
        "\n",
        "# Average the predictions\n",
        "ensemble_preds = (resnet_preds + effnet_preds + custom_preds) / 3\n",
        "final_preds = ensemble_preds.ravel()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cC7l54aFQtDj",
        "outputId": "f5bb4705-953a-4f8b-a131-7e491996ff79"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m12500/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 6ms/step\n",
            "\u001b[1m12500/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 6ms/step\n",
            "\u001b[1m12500/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "image_ids = [int(os.path.splitext(os.path.basename(path))[0]) for path in test_generator.filenames]\n",
        "\n",
        "submission_df = pd.DataFrame({\n",
        "    'id': image_ids,\n",
        "    'label': np.clip(final_preds, 1e-5, 1 - 1e-5)  # Clip for stable log loss\n",
        "}).sort_values('id')\n",
        "\n",
        "submission_df.to_csv('submission_ensemble.csv', index=False)\n",
        "print(\"✅ Ensemble submission saved.\")\n",
        "submission_path = '/content/drive/MyDrive/submission_ensemble.csv'\n",
        "submission_df.to_csv(submission_path, index=False)\n",
        "print(f\"✅ Submission saved to: {submission_path}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOp1NEDuQwMw",
        "outputId": "6027e49e-211a-4b0a-dae9-16ea1afd0eaf"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Ensemble submission saved.\n",
            "✅ Submission saved to: /content/drive/MyDrive/submission_ensemble.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### In this ensemble, I got a loss of around 0.14 again, so did not see any notable improvement."
      ],
      "metadata": {
        "id": "ow1Wu_y6O31C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Again Proceeding with another ensemble method"
      ],
      "metadata": {
        "id": "-7KhH2QVPEA1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - First, we will do 5-fold cross-validation to improve robustness of final model\n",
        " - Changed base model to EfficientNetB0\n",
        " - Created an ensemble of 5 models"
      ],
      "metadata": {
        "id": "sCeAx3AIjx41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# Step 1: Point to the actual image location\n",
        "image_paths = glob.glob('/content/train/train/*.jpg')\n",
        "\n",
        "# Step 2: Create DataFrame with inferred labels\n",
        "data = []\n",
        "for path in image_paths:\n",
        "    fname = os.path.basename(path)\n",
        "    label = 'dog' if \"dog\" in fname.lower() else 'cat'  # Ensure labels are strings\n",
        "    data.append({'filename': path, 'label': label})\n",
        "\n",
        "train_df = pd.DataFrame(data)\n",
        "print(train_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuBneW-qj2m_",
        "outputId": "f21eea14-0764-493d-dfbb-ea66b9abcc45"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                             filename label\n",
            "0  /content/train/train/cat.12147.jpg   cat\n",
            "1   /content/train/train/cat.8322.jpg   cat\n",
            "2   /content/train/train/dog.7215.jpg   dog\n",
            "3   /content/train/train/dog.5234.jpg   dog\n",
            "4   /content/train/train/dog.1390.jpg   dog\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = pd.DataFrame({\n",
        "    'filename': sorted(glob.glob('/content/test/test/*.jpg'))\n",
        "})\n"
      ],
      "metadata": {
        "id": "2uq_DSdsj5Oj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Total images: {len(train_df)}\")\n",
        "print(f\"Dogs: {(train_df['label'] == 'dog').sum()}, Cats: {(train_df['label'] == 'cat').sum()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac0AdFHykI1W",
        "outputId": "b9d3145b-d80a-47ad-8c3d-8d47196d23ee"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images: 25000\n",
            "Dogs: 12500, Cats: 12500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.applications import EfficientNetB3\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import tensorflow.keras.backend as K"
      ],
      "metadata": {
        "id": "i7_fLiJSkrtC"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 5\n",
        "KFOLDS = 5\n",
        "models = []\n",
        "preds = []\n"
      ],
      "metadata": {
        "id": "JsYPECAPktCz"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# K-Fold CV\n",
        "kf = KFold(n_splits=KFOLDS, shuffle=True, random_state=42)\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(train_df)):\n",
        "    print(f\"\\n===== Fold {fold+1}/{KFOLDS} =====\")\n",
        "    K.clear_session()\n",
        "\n",
        "    train_data = train_df.iloc[train_idx].reset_index(drop=True)\n",
        "    val_data = train_df.iloc[val_idx].reset_index(drop=True)\n",
        "\n",
        "    train_gen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        rotation_range=15,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True\n",
        "    ).flow_from_dataframe(\n",
        "        train_data,\n",
        "        x_col='filename', y_col='label',\n",
        "        target_size=IMG_SIZE,\n",
        "        class_mode='binary',\n",
        "        batch_size=BATCH_SIZE\n",
        "    )\n",
        "\n",
        "    val_gen = ImageDataGenerator(rescale=1./255).flow_from_dataframe(\n",
        "        val_data,\n",
        "        x_col='filename', y_col='label',\n",
        "        target_size=IMG_SIZE,\n",
        "        class_mode='binary',\n",
        "        batch_size=BATCH_SIZE\n",
        "    )\n",
        "\n",
        "    # Build model\n",
        "    base_model = EfficientNetB3(include_top=False, weights='imagenet', input_shape=IMG_SIZE + (3,))\n",
        "    x = GlobalAveragePooling2D()(base_model.output)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    output = Dense(1, activation='sigmoid')(x)\n",
        "    model = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "    # Phase 1: freeze base\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "    model.compile(optimizer=Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    model.fit(train_gen, validation_data=val_gen, epochs=EPOCHS,\n",
        "              callbacks=[EarlyStopping(patience=2, restore_best_weights=True)])\n",
        "\n",
        "    # Phase 2: unfreeze base\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = True\n",
        "    model.compile(optimizer=Adam(1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    model.fit(train_gen, validation_data=val_gen, epochs=EPOCHS,\n",
        "              callbacks=[EarlyStopping(patience=2, restore_best_weights=True)])\n",
        "\n",
        "    models.append(model)\n",
        "    print(f\"✅ Fold {fold+1} complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFI_yFWwkyOc",
        "outputId": "d359e3ee-caed-4319-baa9-f7d1ff2e757d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Fold 1/5 =====\n",
            "Found 20000 validated image filenames belonging to 2 classes.\n",
            "Found 5000 validated image filenames belonging to 2 classes.\n",
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb3_notop.h5\n",
            "\u001b[1m43941136/43941136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m273s\u001b[0m 377ms/step - accuracy: 0.5349 - loss: 0.6899 - val_accuracy: 0.5882 - val_loss: 0.6725\n",
            "Epoch 2/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 349ms/step - accuracy: 0.5756 - loss: 0.6763 - val_accuracy: 0.6062 - val_loss: 0.6669\n",
            "Epoch 3/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 349ms/step - accuracy: 0.5873 - loss: 0.6722 - val_accuracy: 0.6044 - val_loss: 0.6641\n",
            "Epoch 4/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 348ms/step - accuracy: 0.5885 - loss: 0.6730 - val_accuracy: 0.6052 - val_loss: 0.6629\n",
            "Epoch 5/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 352ms/step - accuracy: 0.5883 - loss: 0.6705 - val_accuracy: 0.5956 - val_loss: 0.6648\n",
            "Epoch 1/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m352s\u001b[0m 375ms/step - accuracy: 0.7223 - loss: 0.5309 - val_accuracy: 0.9476 - val_loss: 0.1617\n",
            "Epoch 2/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 353ms/step - accuracy: 0.9551 - loss: 0.1264 - val_accuracy: 0.9778 - val_loss: 0.0720\n",
            "Epoch 3/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 352ms/step - accuracy: 0.9709 - loss: 0.0804 - val_accuracy: 0.9722 - val_loss: 0.0711\n",
            "Epoch 4/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 357ms/step - accuracy: 0.9799 - loss: 0.0559 - val_accuracy: 0.9876 - val_loss: 0.0391\n",
            "Epoch 5/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 352ms/step - accuracy: 0.9831 - loss: 0.0468 - val_accuracy: 0.9880 - val_loss: 0.0353\n",
            "✅ Fold 1 complete.\n",
            "\n",
            "===== Fold 2/5 =====\n",
            "Found 20000 validated image filenames belonging to 2 classes.\n",
            "Found 5000 validated image filenames belonging to 2 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 374ms/step - accuracy: 0.5436 - loss: 0.6875 - val_accuracy: 0.6198 - val_loss: 0.6689\n",
            "Epoch 2/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 353ms/step - accuracy: 0.5616 - loss: 0.6802 - val_accuracy: 0.5956 - val_loss: 0.6667\n",
            "Epoch 3/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m219s\u001b[0m 351ms/step - accuracy: 0.5841 - loss: 0.6720 - val_accuracy: 0.5856 - val_loss: 0.6688\n",
            "Epoch 4/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 353ms/step - accuracy: 0.5822 - loss: 0.6735 - val_accuracy: 0.6142 - val_loss: 0.6559\n",
            "Epoch 5/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 354ms/step - accuracy: 0.5810 - loss: 0.6748 - val_accuracy: 0.6056 - val_loss: 0.6600\n",
            "Epoch 1/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m346s\u001b[0m 378ms/step - accuracy: 0.7547 - loss: 0.4902 - val_accuracy: 0.9332 - val_loss: 0.1824\n",
            "Epoch 2/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 356ms/step - accuracy: 0.9570 - loss: 0.1235 - val_accuracy: 0.9778 - val_loss: 0.0616\n",
            "Epoch 3/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 356ms/step - accuracy: 0.9738 - loss: 0.0747 - val_accuracy: 0.9802 - val_loss: 0.0537\n",
            "Epoch 4/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 354ms/step - accuracy: 0.9777 - loss: 0.0591 - val_accuracy: 0.9862 - val_loss: 0.0368\n",
            "Epoch 5/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 353ms/step - accuracy: 0.9838 - loss: 0.0442 - val_accuracy: 0.9872 - val_loss: 0.0329\n",
            "✅ Fold 2 complete.\n",
            "\n",
            "===== Fold 3/5 =====\n",
            "Found 20000 validated image filenames belonging to 2 classes.\n",
            "Found 5000 validated image filenames belonging to 2 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 368ms/step - accuracy: 0.5445 - loss: 0.6872 - val_accuracy: 0.5956 - val_loss: 0.6703\n",
            "Epoch 2/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m219s\u001b[0m 351ms/step - accuracy: 0.5800 - loss: 0.6753 - val_accuracy: 0.6098 - val_loss: 0.6651\n",
            "Epoch 3/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 353ms/step - accuracy: 0.5815 - loss: 0.6722 - val_accuracy: 0.6068 - val_loss: 0.6658\n",
            "Epoch 4/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 349ms/step - accuracy: 0.5870 - loss: 0.6722 - val_accuracy: 0.6072 - val_loss: 0.6640\n",
            "Epoch 5/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 353ms/step - accuracy: 0.5961 - loss: 0.6693 - val_accuracy: 0.6088 - val_loss: 0.6630\n",
            "Epoch 1/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m349s\u001b[0m 382ms/step - accuracy: 0.7640 - loss: 0.4770 - val_accuracy: 0.9634 - val_loss: 0.1178\n",
            "Epoch 2/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 355ms/step - accuracy: 0.9564 - loss: 0.1211 - val_accuracy: 0.9642 - val_loss: 0.0909\n",
            "Epoch 3/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 355ms/step - accuracy: 0.9727 - loss: 0.0769 - val_accuracy: 0.9700 - val_loss: 0.0779\n",
            "Epoch 4/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 353ms/step - accuracy: 0.9784 - loss: 0.0591 - val_accuracy: 0.9848 - val_loss: 0.0445\n",
            "Epoch 5/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 356ms/step - accuracy: 0.9834 - loss: 0.0455 - val_accuracy: 0.9906 - val_loss: 0.0331\n",
            "✅ Fold 3 complete.\n",
            "\n",
            "===== Fold 4/5 =====\n",
            "Found 20000 validated image filenames belonging to 2 classes.\n",
            "Found 5000 validated image filenames belonging to 2 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 375ms/step - accuracy: 0.5393 - loss: 0.6903 - val_accuracy: 0.5868 - val_loss: 0.6687\n",
            "Epoch 2/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 349ms/step - accuracy: 0.5827 - loss: 0.6752 - val_accuracy: 0.6180 - val_loss: 0.6569\n",
            "Epoch 3/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 352ms/step - accuracy: 0.5797 - loss: 0.6732 - val_accuracy: 0.6088 - val_loss: 0.6607\n",
            "Epoch 4/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 353ms/step - accuracy: 0.5937 - loss: 0.6684 - val_accuracy: 0.6170 - val_loss: 0.6573\n",
            "Epoch 1/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m350s\u001b[0m 380ms/step - accuracy: 0.7537 - loss: 0.4863 - val_accuracy: 0.9586 - val_loss: 0.1267\n",
            "Epoch 2/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 357ms/step - accuracy: 0.9579 - loss: 0.1233 - val_accuracy: 0.9714 - val_loss: 0.0751\n",
            "Epoch 3/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 355ms/step - accuracy: 0.9704 - loss: 0.0773 - val_accuracy: 0.9862 - val_loss: 0.0382\n",
            "Epoch 4/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 356ms/step - accuracy: 0.9792 - loss: 0.0576 - val_accuracy: 0.9850 - val_loss: 0.0392\n",
            "Epoch 5/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 357ms/step - accuracy: 0.9819 - loss: 0.0443 - val_accuracy: 0.9884 - val_loss: 0.0302\n",
            "✅ Fold 4 complete.\n",
            "\n",
            "===== Fold 5/5 =====\n",
            "Found 20000 validated image filenames belonging to 2 classes.\n",
            "Found 5000 validated image filenames belonging to 2 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 376ms/step - accuracy: 0.5403 - loss: 0.6886 - val_accuracy: 0.5890 - val_loss: 0.6711\n",
            "Epoch 2/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 353ms/step - accuracy: 0.5786 - loss: 0.6749 - val_accuracy: 0.6098 - val_loss: 0.6612\n",
            "Epoch 3/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m219s\u001b[0m 350ms/step - accuracy: 0.5872 - loss: 0.6704 - val_accuracy: 0.6016 - val_loss: 0.6642\n",
            "Epoch 4/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 354ms/step - accuracy: 0.5848 - loss: 0.6705 - val_accuracy: 0.6102 - val_loss: 0.6586\n",
            "Epoch 5/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 355ms/step - accuracy: 0.5917 - loss: 0.6700 - val_accuracy: 0.6082 - val_loss: 0.6595\n",
            "Epoch 1/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m345s\u001b[0m 379ms/step - accuracy: 0.7702 - loss: 0.4696 - val_accuracy: 0.9610 - val_loss: 0.1220\n",
            "Epoch 2/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m224s\u001b[0m 358ms/step - accuracy: 0.9545 - loss: 0.1170 - val_accuracy: 0.9848 - val_loss: 0.0531\n",
            "Epoch 3/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 361ms/step - accuracy: 0.9711 - loss: 0.0775 - val_accuracy: 0.9668 - val_loss: 0.0845\n",
            "Epoch 4/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 356ms/step - accuracy: 0.9774 - loss: 0.0582 - val_accuracy: 0.9882 - val_loss: 0.0377\n",
            "Epoch 5/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 356ms/step - accuracy: 0.9857 - loss: 0.0408 - val_accuracy: 0.9838 - val_loss: 0.0494\n",
            "✅ Fold 5 complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "test_gen = ImageDataGenerator(rescale=1./255).flow_from_dataframe(\n",
        "    test_df,\n",
        "    x_col='filename',\n",
        "    target_size=IMG_SIZE,\n",
        "    class_mode=None,\n",
        "    shuffle=False,\n",
        "    batch_size=1\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3InTM3SSJNyx",
        "outputId": "50733f42-1daf-4010-af73-6af60bef659c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 12500 validated image filenames.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "all_preds = []\n",
        "\n",
        "for i, model in enumerate(models):\n",
        "    print(f\"Predicting with model {i+1}\")\n",
        "    test_gen.reset()\n",
        "    preds = model.predict(test_gen, verbose=0)\n",
        "    all_preds.append(preds)\n",
        "\n",
        "# Average predictions across all folds\n",
        "final_preds = np.mean(all_preds, axis=0).ravel()\n",
        "\n",
        "# Clip values to avoid log loss issues\n",
        "final_preds = np.clip(final_preds, 1e-5, 1 - 1e-5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYNnt_lPJUzv",
        "outputId": "a46679d3-b83b-444d-f719-d2c4cb7c50c7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting with model 1\n",
            "Predicting with model 2\n",
            "Predicting with model 3\n",
            "Predicting with model 4\n",
            "Predicting with model 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract just the image IDs from filenames\n",
        "test_ids = [int(f.split('/')[-1].split('.')[0]) for f in test_df['filename']]\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    'id': test_ids,\n",
        "    'label': final_preds\n",
        "}).sort_values('id')\n",
        "\n",
        "submission.to_csv('submission_efficientNet.csv', index=False)\n",
        "print(\"submission.csv is ready!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6WwcG9jJYGc",
        "outputId": "23f69ac4-672c-48a0-c35d-e7b49e01576b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "submission.csv is ready!\n"
          ]
        }
      ]
    }
  ]
}